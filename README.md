# aia.nvim
## Description
AIA (AI Assistant) is a Neovim Plugin that acts as an AI coding assistant for different projects. The Neovim plug connects to a custom c++ proxy server through a TCP connection. The c++ proxy server routes the request to a LLM API and then sends the response back to the Neovim Plugin.

## Neovim Plugin
There are two main parts to the Neovim plugin. The first part is project creation/ AI assistant initialization. The idea is to have a different AI agent for each project. When initializing a new AI assistant, you describe the overview of the project. A project can only be created within a GIT repo. If the project is not in a GIT repo, then the user is told so. The second part of the Neovim plugin is the actual interaction of the AI Assistant. When sending a prompt to the AI Assistant, the Neovim plugin gathers different project contexts to send to the AI Assistant. The contexts include: Harpoon file signatures and git diff (up to a max number of chars). The file structure of the project was included before but seemed unnecessary. The interaction with the AI assistant will be displayed in markdown for text styling. The AI assistant buffer is not closed until Neovim is closed so the past conversion in the Neovim session is kept.

## C++ Proxy Server
The C++ proxy server (a 10 year old laptop running an arch linux server instance) processes the request from the Neovim plugin. The request is either a project request or an AI Prompt request. If it is a project request then the proxy saves the new project in a sqlite database to use later on. If it is a prompt request then it grabs a valid LLM API and sends the request to it. The idea is that the c++ proxy would have many different LLM APIs that it could route to if the rate limit was met on any of the LLMs. This is because the free version is being used for the LLM APIs and they have much smaller rate limits than the paid for LLM APIs. These API endpoints are “stateless” so they don’t remember past conversion history. This is why we store the project context in a sqlite database and add it to every request. After the response comes back from the LLM API, the conversion history is stored in the database. This conversion history is then used to create a summary using the textrank algorithm on the past N converstions. We save that summary and add it to the next request to add some state to the stateless API. The textrank algorithm could probably be done better but it at least tries to create a summary of the past conversations.
